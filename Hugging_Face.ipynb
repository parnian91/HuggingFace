{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNOOc+WTmCfU8Dhwk0/VwUC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parnian91/HuggingFace/blob/main/Hugging_Face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5WIUL7v1uwY",
        "outputId": "9707327b-ebb2-4820-e5cc-a203c6baf08e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "J_dqdtqqAIXU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentiment_Analysis"
      ],
      "metadata": {
        "id": "Mmcy7EJsKy52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline('sentiment-analysis')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LYl1bmI_3TB",
        "outputId": "c0cc2b74-02cd-4c55-c87b-93363a31ce1a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier('I hate that movie and the cast was bad ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dfglg27U_3Vx",
        "outputId": "fe62c7a9-8fc8-4a43-a56b-fff8039fb9be"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9996799230575562}]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier2 = pipeline('sentiment-analysis', model='bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkQyc6x1ALrp",
        "outputId": "0de4ac45-1068-42da-bc8d-d7323c916720"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier2('I loved that movie and the cast was good')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPQ-yYEsALuI",
        "outputId": "38a607dd-6ab1-4ac0-f3eb-3538ded2dc7e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'LABEL_0', 'score': 0.5396947264671326}]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = classifier([\"we are very happy to show you the Transformer library.\",\n",
        "                      \"We hope you don't hate it.\"])\n",
        "for i in results:\n",
        "  print(f\"{i['label']}, {round(i['score'], 2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TkwRBXpALwq",
        "outputId": "3d3e83e5-a5b2-4a09-fa57-5199c33773d7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE, 1.0\n",
            "NEGATIVE, 0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generator"
      ],
      "metadata": {
        "id": "24mVJ-J0D4rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('text-generation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxki--AS12cg",
        "outputId": "d8f99453-198e-42ba-8b30-99a5a1f74c88"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generator(\"As far as I am concerned, I will\",))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ3yq_lJD-G-",
        "outputId": "83946891-dcfa-421e-e3de-32a31aa038b3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"As far as I am concerned, I will do so in whatever capacity [if necessary]. It would be easy to understand, but I don't think I need to say anything at this moment.\\n\\nI am sorry, but I have to leave\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fill-Mask"
      ],
      "metadata": {
        "id": "soaigwVgFEGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline for fill-mask task\n",
        "UnMasker = pipeline(task=\"fill-mask\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkM9XJy_EPkm",
        "outputId": "fb079501-987e-4827-fea5-0779143874b2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "pprint(UnMasker(f\"HuggingFace is creating a {UnMasker.tokenizer.mask_token} that the community usee to solve NLP tasks\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6QSpY-WEPnZ",
        "outputId": "9fb2a339-0c38-40fc-bd11-23966c8ab2f2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.24226777255535126,\n",
            "  'sequence': 'HuggingFace is creating a tool that the community usee to solve '\n",
            "              'NLP tasks',\n",
            "  'token': 3944,\n",
            "  'token_str': ' tool'},\n",
            " {'score': 0.08202940970659256,\n",
            "  'sequence': 'HuggingFace is creating a framework that the community usee to '\n",
            "              'solve NLP tasks',\n",
            "  'token': 7208,\n",
            "  'token_str': ' framework'},\n",
            " {'score': 0.04256993532180786,\n",
            "  'sequence': 'HuggingFace is creating a bot that the community usee to solve '\n",
            "              'NLP tasks',\n",
            "  'token': 14084,\n",
            "  'token_str': ' bot'},\n",
            " {'score': 0.03987973555922508,\n",
            "  'sequence': 'HuggingFace is creating a library that the community usee to '\n",
            "              'solve NLP tasks',\n",
            "  'token': 5560,\n",
            "  'token_str': ' library'},\n",
            " {'score': 0.034820228815078735,\n",
            "  'sequence': 'HuggingFace is creating a plugin that the community usee to '\n",
            "              'solve NLP tasks',\n",
            "  'token': 43201,\n",
            "  'token_str': ' plugin'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NER"
      ],
      "metadata": {
        "id": "q1Jf6YtSG5E1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_pipeline = pipeline(task=\"ner\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-MBMR6kEPqF",
        "outputId": "1eff2254-7b8a-4e58-d519-dfae3a86a24d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text for NER\n",
        "text = \"Apple Inc. is planning to open a new store in San Francisco.\"\n",
        "\n",
        "# Use the NER pipeline to extract named entities\n",
        "ner_results = ner_pipeline(text)\n",
        "\n",
        "# Print the named entities and their labels\n",
        "for result in ner_results:\n",
        "    print(f\"Entity: {result['word']}, Label: {result['entity']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PoCQsjHD-J9",
        "outputId": "4c98ec26-3b63-4fe5-a0b6-2a84e3db0f40"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Apple, Label: I-ORG\n",
            "Entity: Inc, Label: I-ORG\n",
            "Entity: San, Label: I-LOC\n",
            "Entity: Francisco, Label: I-LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question-Answering"
      ],
      "metadata": {
        "id": "daiExwsMHe-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline for question-answering\n",
        "qa_pipeline = pipeline(task=\"question-answering\")\n",
        "\n",
        "# Input context and question\n",
        "context = \"Hugging Face Transformers is a natural language processing library.\"\n",
        "question = \"What is Hugging Face Transformers?\"\n",
        "\n",
        "# Use the question-answering pipeline to get the answer\n",
        "answer = qa_pipeline(question=question, context=context)\n",
        "\n",
        "# Print the answer\n",
        "print(f\"Answer: {answer['answer']}, Score: {answer['score']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9cfJ6wlHkZ6",
        "outputId": "9fd651c9-a0df-4b69-cae2-7a1e33ad3c1c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: a natural language processing library, Score: 0.5033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summarization"
      ],
      "metadata": {
        "id": "2DuKlpEQIJMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline for text summarization\n",
        "summarizer = pipeline(task=\"summarization\", model=\"t5-base\")"
      ],
      "metadata": {
        "id": "x7Szma10Hkcr"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text for summarization\n",
        "text_to_summarize = \"\"\"\n",
        "Hugging Face Transformers is a powerful natural language processing library.\n",
        "It provides a variety of pre-trained models for tasks like text summarization.\n",
        "The library is easy to use and widely adopted in the NLP community.\n",
        "\"\"\"\n",
        "\n",
        "# Use the summarization pipeline to generate a summary\n",
        "summary = summarizer(text_to_summarize, max_length=50, min_length=20, length_penalty=2.0, num_beams=4)\n",
        "\n",
        "# Print the generated summary\n",
        "print(\"Generated Summary:\", summary[0][\"summary_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCS-HB_kHkfJ",
        "outputId": "84399b8d-c199-42f4-a039-16f8c0b3d972"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: the library is easy to use and widely adopted in the NLP community . it provides a variety of pre-trained models for tasks like text summarization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Translation"
      ],
      "metadata": {
        "id": "Wy9G7fA0Ii-K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "covLpno0JvJZ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "01nz5qRd12fS"
      },
      "execution_count": 49,
      "outputs": []
    }
  ]
}